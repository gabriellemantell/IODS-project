# Logistic Regression

```{r}
# Load libraries
library(tidyr); library(dplyr); library(ggplot2); library(GGally)

# Import data
alc <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/alc.txt ", sep=",", header=TRUE)
```

The data used in this analysis is taken from school reports and questionnaires collected in 2008. It approaches student achievement in secondary education in two Portuguese schools and regard performance in two subjects: math (mat) and Portuguese language (por). The data has 382 observations and 35 variables.


```{r}
str(alc)
dim(alc)
```

#### Analysis of variables
For this analysis I am interested in the following four variables and their relationship with alcohol consumption: sex (male or female), failures (number of past class failures), Dalc (workday alcohol consumption) and absences (number of school absences). I expect that failures, Dalc and absences will be indicators of a student who consumes higher levels of alcohol (high_use which is TRUE for students for which 'alc_use' is greater than 2 and FALSE otherwise). Additionally, I think that male students will be more likely to drink than female students.

```{r}
data(alc, package = "COUNT")
attach(alc)
table(high_use, sex)
table(high_use, failures)
table(high_use, Dalc)
table(high_use, absences)

```



```{r}
g2 <- ggplot(alc, aes(x = high_use, y = failures, col = sex))
g2 + geom_boxplot() + ylab("failures") + ggtitle("Student failures by alcohol consumption and sex")

g3 <- ggplot(alc, aes(x = high_use, y = Dalc, col = sex))
g3 + geom_boxplot() + ylab("Dalc") + ggtitle("Student work day alcohol consumption by alcohol consumption and sex")

g4 <- ggplot(alc, aes(x = high_use, y = absences, col = sex))
g4 + geom_boxplot() + ylab("absences") + ggtitle("Student absences by alcohol consumption and sex")

```

```{r}
gather(alc) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()
```

Nearly twice as many male students (41) than female students (71) were high_use which supports my initial hypothesis. The correlation between failures and absences is less clear as the figures for the predictor values decrease with each subsequent increase in high_use. Perhaps most interesting is that the Dalc is highest when high_use is 2, however from there it decreases as high_use increase. This is surprising.

### Logistic regression
```{r}
# find the model with glm()
m <- glm(high_use ~ failures + absences + sex + Dalc, data = alc, family = "binomial")

# print out a summary of the model
summary(m)

# print out the coefficients of the model
coef(m)
```

```{r}
# compute odds ratios (OR)
OR <- coef(m) %>% exp

# compute confidence intervals (CI)
CI <- confint(m) %>% exp

# print out the odds ratios with their confidence intervals
cbind(OR, CI)

```

#### Analysis of the model
Of the four predictor variables, absences (p-value = 0.006) and Dalc (p-value = <0.001). This further supports my hypothesis that absences and Dalc are predictors of students who will classify as high_use. On the other hand, failure and gender are not statistically significant predictors.

The odds ratio of this model implies that a 1 unit increase in Dalc increases the odds of a student classifying as high_use by 4.45, while a 1 unit increase in absences increases the odds of a student classifying as high_use by 1.07. These results supports my original intuition, specifically that drinking in the day (Dalc) is correlated with high_use.


#### Make predictions
```{r}
# fit the model
m <- glm(high_use ~ absences + Dalc, data = alc, family = "binomial")

# predict() the probability of high_use
probabilities <- predict(m, type = "response")

# add the predicted probabilities to 'alc'
alc <- mutate(alc, probability = probabilities)

# use the probabilities to make a prediction of high_use
alc <- mutate(alc, prediction = probability > 0.5)

# see the last ten original classes, predicted probabilities, and class predictions
select(alc, absences, Dalc, high_use, probability, prediction) %>% tail(10)

# tabulate the target variable versus the predictions
table(high_use = alc$high_use, prediction = probabilities > 0.5)
```

```{r}
# initialize a plot of 'high_use' versus 'probability' in 'alc'
g <- ggplot(alc, aes(x = probability, y = high_use, col = prediction))

# define the geom as points and draw the plot
g + geom_point()

# tabulate the target variable versus the predictions
table(high_use = alc$high_use, prediction = alc$prediction) %>% prop.table() %>% addmargins()
```

#### Accuracy of model
```{r}
# define a loss function (mean prediction error)
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

# call loss_func to compute the average number of wrong predictions in the (training) data
loss_func(class = alc$high_use, prob = alc$probability)
```


```{r}
# define a loss function (average prediction error)
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

# compute the average number of wrong predictions in the (training) data
loss_func(class = alc$high_use, prob = alc$probability)
```


#### 10 fold cross-validation
```{r}
# K-fold cross-validation
library(boot)
cv <- cv.glm(data = alc, cost = loss_func, glmfit = m, K = 10)

# average number of wrong predictions in the cross validation
cv$delta[1]
```


### Analysis of predictions
In this model, 9% of the individuals were inaccurately classified. This figure remains the same even after conducting a 10-fold cross-validation. I would propose that a 91% prediction rate is a significant improvement on the results of a simple guessing strategy.
